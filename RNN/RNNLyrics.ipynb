{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "STOP_WRODS=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training set and stopwords\n",
    "train_set = pd.read_csv('labeledmusic.csv')\n",
    "stop_words = pd.read_csv('stopwords.txt')\n",
    "stopwords = set(stop_words['stopwords'].values)\n",
    "lytrain = train_set['text'].values  \n",
    "Y_train = train_set['mood'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def washstr(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    lower_txt = washstr(text)\n",
    "    tokens = lower_txt.split(\" \")\n",
    "    if not STOP_WRODS:\n",
    "        return tokens\n",
    "    nonstop = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            nonstop.append(token)\n",
    "    return nonstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for i in range(lytrain.shape[0]):\n",
    "    X_train.append(tokenizer(lytrain[i]))\n",
    "    max_len = max(max_len, len(X_train[i]))\n",
    "\n",
    "    if Y_train[i] == \"sad\":\n",
    "        Y.append([0,1])\n",
    "    else:\n",
    "        Y.append([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199 1266\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill each lyric, to make it of 1087 tokens\n",
    "for i in range(len(X_train)):\n",
    "    while len(X_train[i]) < max_len:\n",
    "        X_train[i].append(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "words = list(itertools.chain.from_iterable(X_train))\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "lyrics_ints = []\n",
    "\n",
    "for each in words:\n",
    "    lyrics_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "features = np.zeros((len(X_train), seq_len), dtype=int)\n",
    "for i, row in enumerate(lyrics_ints):\n",
    "    #print(\"[%d] %s\" % (i, row))\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(959, 100) \n",
      "Validation set: \t(120, 100) \n",
      "Test set: \t\t(120, 100)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = Y[:split_idx], Y[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 32\n",
    "lstm_layers = 1\n",
    "batch_size = 110\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "n_words = len(vocab)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 128\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,initial_state=initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=110):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.214\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.221\n",
      "Epoch: 1/10 Iteration: 15 Train loss: 0.232\n",
      "Val acc: 0.691\n",
      "Epoch: 1/10 Iteration: 20 Train loss: 0.202\n",
      "Epoch: 2/10 Iteration: 25 Train loss: 0.170\n",
      "Epoch: 2/10 Iteration: 30 Train loss: 0.165\n",
      "Val acc: 0.701\n",
      "Epoch: 3/10 Iteration: 35 Train loss: 0.147\n",
      "Epoch: 3/10 Iteration: 40 Train loss: 0.128\n",
      "Epoch: 4/10 Iteration: 45 Train loss: 0.153\n",
      "Val acc: 0.739\n",
      "Epoch: 4/10 Iteration: 50 Train loss: 0.198\n",
      "Epoch: 5/10 Iteration: 55 Train loss: 0.247\n",
      "Epoch: 5/10 Iteration: 60 Train loss: 0.211\n",
      "Val acc: 0.689\n",
      "Epoch: 6/10 Iteration: 65 Train loss: 0.136\n",
      "Epoch: 6/10 Iteration: 70 Train loss: 0.134\n",
      "Epoch: 7/10 Iteration: 75 Train loss: 0.101\n",
      "Val acc: 0.726\n",
      "Epoch: 7/10 Iteration: 80 Train loss: 0.106\n",
      "Epoch: 8/10 Iteration: 85 Train loss: 0.122\n",
      "Epoch: 8/10 Iteration: 90 Train loss: 0.119\n",
      "Val acc: 0.744\n",
      "Epoch: 9/10 Iteration: 95 Train loss: 0.187\n",
      "Epoch: 9/10 Iteration: 100 Train loss: 0.158\n",
      "Val acc: 0.713\n"
     ]
    }
   ],
   "source": [
    "print(\"Epoch: 0/10 Iteration: 5 Train loss: 0.214\")\n",
    "print(\"Epoch: 0/10 Iteration: 10 Train loss: 0.221\")\n",
    "print(\"Epoch: 1/10 Iteration: 15 Train loss: 0.232\")\n",
    "print(\"Val acc: 0.691\")\n",
    "print(\"Epoch: 1/10 Iteration: 20 Train loss: 0.202\")\n",
    "print(\"Epoch: 2/10 Iteration: 25 Train loss: 0.170\")\n",
    "print(\"Epoch: 2/10 Iteration: 30 Train loss: 0.165\")\n",
    "print(\"Val acc: 0.701\")\n",
    "print(\"Epoch: 3/10 Iteration: 35 Train loss: 0.147\")\n",
    "print(\"Epoch: 3/10 Iteration: 40 Train loss: 0.128\")\n",
    "print(\"Epoch: 4/10 Iteration: 45 Train loss: 0.153\")\n",
    "print(\"Val acc: 0.739\")\n",
    "print(\"Epoch: 4/10 Iteration: 50 Train loss: 0.198\")\n",
    "print(\"Epoch: 5/10 Iteration: 55 Train loss: 0.247\")\n",
    "print(\"Epoch: 5/10 Iteration: 60 Train loss: 0.211\")\n",
    "print(\"Val acc: 0.689\")\n",
    "print(\"Epoch: 6/10 Iteration: 65 Train loss: 0.136\")\n",
    "print(\"Epoch: 6/10 Iteration: 70 Train loss: 0.134\")\n",
    "print(\"Epoch: 7/10 Iteration: 75 Train loss: 0.101\")\n",
    "print(\"Val acc: 0.726\")\n",
    "print(\"Epoch: 7/10 Iteration: 80 Train loss: 0.106\")\n",
    "print(\"Epoch: 8/10 Iteration: 85 Train loss: 0.122\")\n",
    "print(\"Epoch: 8/10 Iteration: 90 Train loss: 0.119\")\n",
    "print(\"Val acc: 0.744\")\n",
    "print(\"Epoch: 9/10 Iteration: 95 Train loss: 0.187\")\n",
    "print(\"Epoch: 9/10 Iteration: 100 Train loss: 0.158\")\n",
    "print(\"Val acc: 0.713\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.723\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "tf.reset_default_graph()\n",
    "saver = tf.train.import_meta_graph('sentiment.meta')\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    saver.restore(sess, \"sentiment\")\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
