{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "ALLOW_SOFT_PLACEMENT=True\n",
    "BATCH_SIZE=64\n",
    "DEV_SAMPLE_PERCENTAGE=0.1\n",
    "DROPOUT_KEEP_PROB=0.5\n",
    "EMBEDDING_DIM=64\n",
    "EVALUATE_EVERY=15\n",
    "FILTER_SIZES=[3,4,5]\n",
    "L2_REG_LAMBDA=0.001\n",
    "LOG_DEVICE_PLACEMENT=False\n",
    "NUM_EPOCHS=20\n",
    "NUM_FILTERS=128\n",
    "fname = \"w2vmodel\" #w2vmodel128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def washstr(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    lower_txt = washstr(text)\n",
    "    return lower_txt.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extfeat(X_train):\n",
    "    neg = pd.read_csv('negative-words.txt')['neg'].values\n",
    "    pos = pd.read_csv('positive-words.txt')['pos'].values\n",
    "    neg = set(neg)\n",
    "    pos = set(pos)\n",
    "    \n",
    "    ext = []\n",
    "    for i in range(len(X_train)):\n",
    "        p = 0.0\n",
    "        n = 0.0\n",
    "        total = 0.0\n",
    "        for j in X_train[i]:\n",
    "            if j != \" \":\n",
    "                total += 1\n",
    "                if j in pos:\n",
    "                    p += 1\n",
    "                if j in neg:\n",
    "                    n += 1\n",
    "        if n == 0:\n",
    "            ratio = p\n",
    "        else:\n",
    "            ratio = p/n\n",
    "        ext.append([p,n,p/total,n/total,ratio])\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def word2vec(X_train):\n",
    "    X = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X.append(tokenizer(X_train[i]))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        while len(X[i]) < max_len:\n",
    "            X[i].append(\" \")\n",
    "        \n",
    "    model = gensim.models.Word2Vec(X, min_count=1,size=EMBEDDING_DIM)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwvec(X_train,model):\n",
    "    x = []\n",
    "    for i in range(len(X_train)):\n",
    "        x.append(model.wv[X_train[i]])\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    print(data_size)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: DROPOUT_KEEP_PROB\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "def dev_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('labeledmusic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lytrain = train_set['text'].values \n",
    "\n",
    "y_train = train_set['mood'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,) (1199,)\n"
     ]
    }
   ],
   "source": [
    "print(lytrain.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for i in range(lytrain.shape[0]):\n",
    "    X_train.append(tokenizer(lytrain[i]))\n",
    "    max_len = max(max_len, len(X_train[i]))\n",
    "\n",
    "    if y_train[i] == \"sad\":\n",
    "        Y.append([0,1])\n",
    "    else:\n",
    "        Y.append([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199 1266\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill each lyric, to make it of 1087 tokens\n",
    "for i in range(len(X_train)):\n",
    "    while len(X_train[i]) < max_len:\n",
    "        X_train[i].append(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if len(X_train[i]) != max_len:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,)\n"
     ]
    }
   ],
   "source": [
    "print(lytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since the dataset is too small, word vector may not be good, so use a greater lyric set to generate w2v model\n",
    "# unlabeled = pd.read_csv('songdata.csv')\n",
    "# w2vtran = unlabeled['text'].values\n",
    "# w2vtran = np.row_stack((w2vtran.reshape(-1,1),lytrain.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = word2vec(w2vtran[:,0])\n",
    "# model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2vtran.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1266, 64)\n"
     ]
    }
   ],
   "source": [
    "x_ = getwvec(X_train,model)\n",
    "print(x_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra = extfeat(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 5)\n"
     ]
    }
   ],
   "source": [
    "# extra = np.array(extra)\n",
    "# print(extra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ = x_.reshape(-1,x_.shape[1]*x_.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.column_stack((x_,extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA \n",
    "# pca=PCA(n_components=0.998)\n",
    "# x = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1266, 64) (1199, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yTrain/Dev split: 1080/119\n",
      "xTrain/Dev split: 1080/119\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(DEV_SAMPLE_PERCENTAGE * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"yTrain/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "print(\"xTrain/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "1080\n",
      "64\n",
      "2017-12-19T19:15:04.989410: step 1, loss 9.99912, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:15:07.591485: step 2, loss 8.78053, acc 0.625\n",
      "64\n",
      "2017-12-19T19:15:10.521677: step 3, loss 5.7041, acc 0.453125\n",
      "64\n",
      "2017-12-19T19:15:12.791178: step 4, loss 8.1609, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:15:15.013920: step 5, loss 7.83445, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:15:17.420346: step 6, loss 9.73429, acc 0.40625\n",
      "64\n",
      "2017-12-19T19:15:20.058216: step 7, loss 7.60429, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:15:22.263542: step 8, loss 9.38398, acc 0.4375\n",
      "64\n",
      "2017-12-19T19:15:24.804120: step 9, loss 4.3338, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:15:27.624168: step 10, loss 7.45454, acc 0.484375\n",
      "64\n",
      "2017-12-19T19:15:30.724552: step 11, loss 7.18542, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:15:33.847141: step 12, loss 8.20557, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:15:36.604880: step 13, loss 6.82364, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:15:39.699893: step 14, loss 7.39747, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:15:42.958360: step 15, loss 6.88104, acc 0.453125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:15:44.480464: step 15, loss 4.94172, acc 0.478992\n",
      "\n",
      "64\n",
      "2017-12-19T19:15:46.866961: step 16, loss 7.96427, acc 0.5\n",
      "56\n",
      "2017-12-19T19:15:49.247034: step 17, loss 8.27805, acc 0.464286\n",
      "64\n",
      "2017-12-19T19:15:51.619905: step 18, loss 3.78693, acc 0.671875\n",
      "64\n",
      "2017-12-19T19:15:53.976252: step 19, loss 9.51908, acc 0.421875\n",
      "64\n",
      "2017-12-19T19:15:56.393598: step 20, loss 5.44197, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:15:59.120566: step 21, loss 5.66346, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:16:01.864191: step 22, loss 6.21229, acc 0.4375\n",
      "64\n",
      "2017-12-19T19:16:04.189136: step 23, loss 4.62591, acc 0.6875\n",
      "64\n",
      "2017-12-19T19:16:06.838839: step 24, loss 5.36347, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:16:09.309656: step 25, loss 6.85486, acc 0.453125\n",
      "64\n",
      "2017-12-19T19:16:11.504416: step 26, loss 3.9241, acc 0.65625\n",
      "64\n",
      "2017-12-19T19:16:13.653832: step 27, loss 5.36987, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:16:15.823289: step 28, loss 4.30986, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:16:18.123085: step 29, loss 5.45743, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:16:20.448316: step 30, loss 7.6686, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:16:22.012188: step 30, loss 4.31499, acc 0.478992\n",
      "\n",
      "64\n",
      "2017-12-19T19:16:24.802385: step 31, loss 6.9258, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:16:27.028741: step 32, loss 5.69139, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:16:29.562759: step 33, loss 2.96212, acc 0.703125\n",
      "56\n",
      "2017-12-19T19:16:31.500938: step 34, loss 6.30366, acc 0.535714\n",
      "64\n",
      "2017-12-19T19:16:33.774600: step 35, loss 2.77069, acc 0.6875\n",
      "64\n",
      "2017-12-19T19:16:36.637982: step 36, loss 3.97672, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:16:40.571376: step 37, loss 4.50178, acc 0.484375\n",
      "64\n",
      "2017-12-19T19:16:43.965579: step 38, loss 6.861, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:16:46.653406: step 39, loss 5.04625, acc 0.5625\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from text_cnn import TextCNN\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=ALLOW_SOFT_PLACEMENT,\n",
    "      log_device_placement=LOG_DEVICE_PLACEMENT)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            filter_sizes=FILTER_SIZES,\n",
    "            num_filters=NUM_FILTERS,\n",
    "            l2_reg_lambda=L2_REG_LAMBDA)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), BATCH_SIZE, NUM_EPOCHS)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            print(len(x_batch))\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev)\n",
    "                print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
