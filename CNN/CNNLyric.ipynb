{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "ALLOW_SOFT_PLACEMENT=True\n",
    "BATCH_SIZE=64\n",
    "DEV_SAMPLE_PERCENTAGE=0.1\n",
    "DROPOUT_KEEP_PROB=0.5\n",
    "EMBEDDING_DIM=64\n",
    "EVALUATE_EVERY=15\n",
    "FILTER_SIZES=5\n",
    "L2_REG_LAMBDA=0.001\n",
    "LOG_DEVICE_PLACEMENT=False\n",
    "NUM_EPOCHS=20\n",
    "NUM_FILTERS=256\n",
    "STOP_WRODS=False\n",
    "fname = \"w2vmodel\" #w2vmodel128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def washstr(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    lower_txt = washstr(text)\n",
    "    tokens = lower_txt.split(\" \")\n",
    "    if not STOP_WRODS:\n",
    "        return tokens\n",
    "    nonstop = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            nonstop.append(token)\n",
    "    return nonstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extfeat(X_train):\n",
    "    neg = pd.read_csv('negative-words.txt')['neg'].values\n",
    "    pos = pd.read_csv('positive-words.txt')['pos'].values\n",
    "    neg = set(neg)\n",
    "    pos = set(pos)\n",
    "    \n",
    "    ext = []\n",
    "    for i in range(len(X_train)):\n",
    "        p = 0.0\n",
    "        n = 0.0\n",
    "        total = 0.0\n",
    "        for j in X_train[i]:\n",
    "            if j != \" \":\n",
    "                total += 1\n",
    "                if j in pos:\n",
    "                    p += 1\n",
    "                if j in neg:\n",
    "                    n += 1\n",
    "        if n == 0:\n",
    "            ratio = p\n",
    "        else:\n",
    "            ratio = p/n\n",
    "        ext.append([p,n,p/total,n/total,ratio])\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def word2vec(X_train):\n",
    "    X = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X.append(tokenizer(X_train[i]))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        while len(X[i]) < max_len:\n",
    "            X[i].append(\" \")\n",
    "        \n",
    "    model = gensim.models.Word2Vec(X, min_count=1,size=EMBEDDING_DIM)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwvec(X_train,model):\n",
    "    x = []\n",
    "    for i in range(len(X_train)):\n",
    "        x.append(model.wv[X_train[i]])\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    print(data_size)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: DROPOUT_KEEP_PROB\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "def dev_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('labeledmusic.csv')\n",
    "stop_words = pd.read_csv('stopwords.txt')\n",
    "stopwords = set(stop_words['stopwords'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me', 'should', 'here', 'we', 'herself', 'most', 'between', 'themselves', 'how', 'the', 'are', 't', 'having', 'with', 'my', 'why', 'and', 'because', 'your', 'until', 'then', 'any', 'them', 'doing', 'i', 'more', 'yourselves', 'just', 'few', 'been', 'they', 'out', 'have', 'to', 'but', 'during', 'has', 'both', 'of', 'not', 'on', 'ourselves', 'am', 'a', 'what', 'other', 'nor', 'hers', 'while', 'had', 'do', 'yourself', 'myself', 'all', 'each', 'than', 'for', 'those', 'will', 'such', 'against', 'over', 'under', 'can', 'their', 'some', 'don', 'him', 'she', 'be', 'when', 'who', 'these', 'it', 'above', 'that', 'again', 'too', 'through', 'were', 'off', 'itself', 'in', 'ours', 'up', 'there', 'down', 'this', 'does', 'only', 'so', 'his', 'its', 'or', 'same', 'now', 's', 'further', 'at', 'did', 'below', 'very', 'own', 'her', 'as', 'an', 'theirs', 'he', 'whom', 'if', 'which', 'you', 'no', 'after', 'is', 'once', 'into', 'where', 'was', 'himself', 'by', 'from', 'yours', 'our', 'being', 'about', 'before'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lytrain = train_set['text'].values \n",
    "\n",
    "y_train = train_set['mood'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,) (1199,)\n"
     ]
    }
   ],
   "source": [
    "print(lytrain.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for i in range(lytrain.shape[0]):\n",
    "    X_train.append(tokenizer(lytrain[i]))\n",
    "    max_len = max(max_len, len(X_train[i]))\n",
    "\n",
    "    if y_train[i] == \"sad\":\n",
    "        Y.append([0,1])\n",
    "    else:\n",
    "        Y.append([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199 1266\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill each lyric, to make it of 1087 tokens\n",
    "for i in range(len(X_train)):\n",
    "    while len(X_train[i]) < max_len:\n",
    "        X_train[i].append(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if len(X_train[i]) != max_len:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,)\n"
     ]
    }
   ],
   "source": [
    "print(lytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since the dataset is too small, word vector may not be good, so use a greater lyric set to generate w2v model\n",
    "# unlabeled = pd.read_csv('songdata.csv')\n",
    "# w2vtran = unlabeled['text'].values\n",
    "# w2vtran = np.row_stack((w2vtran.reshape(-1,1),lytrain.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = word2vec(w2vtran[:,0])\n",
    "# model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2vtran.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1266, 64)\n"
     ]
    }
   ],
   "source": [
    "x_ = getwvec(X_train,model)\n",
    "print(x_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra = extfeat(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra = np.array(extra)\n",
    "# print(extra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ = x_.reshape(-1,x_.shape[1]*x_.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.column_stack((x_,extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA \n",
    "# pca=PCA(n_components=0.998)\n",
    "# x = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1266, 64) (1199, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yTrain/Dev split: 1080/119\n",
      "xTrain/Dev split: 1080/119\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(DEV_SAMPLE_PERCENTAGE * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"yTrain/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "print(\"xTrain/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "1080\n",
      "64\n",
      "2017-12-19T22:58:53.558926: step 1, loss 5.76553, acc 0.5625\n",
      "64\n",
      "2017-12-19T22:58:55.033590: step 2, loss 10.4408, acc 0.421875\n",
      "64\n",
      "2017-12-19T22:58:56.454659: step 3, loss 4.64553, acc 0.609375\n",
      "64\n",
      "2017-12-19T22:58:57.885722: step 4, loss 5.55524, acc 0.546875\n",
      "64\n",
      "2017-12-19T22:58:59.320236: step 5, loss 10.1912, acc 0.46875\n",
      "64\n",
      "2017-12-19T22:59:00.729480: step 6, loss 6.92504, acc 0.484375\n",
      "64\n",
      "2017-12-19T22:59:02.162140: step 7, loss 7.32518, acc 0.609375\n",
      "64\n",
      "2017-12-19T22:59:03.584053: step 8, loss 4.45539, acc 0.578125\n",
      "64\n",
      "2017-12-19T22:59:05.008478: step 9, loss 4.93421, acc 0.578125\n",
      "64\n",
      "2017-12-19T22:59:06.434681: step 10, loss 9.62494, acc 0.40625\n",
      "64\n",
      "2017-12-19T22:59:07.856183: step 11, loss 9.72223, acc 0.484375\n",
      "64\n",
      "2017-12-19T22:59:09.254312: step 12, loss 6.81853, acc 0.5625\n",
      "64\n",
      "2017-12-19T22:59:10.669582: step 13, loss 8.11634, acc 0.453125\n",
      "64\n",
      "2017-12-19T22:59:12.077434: step 14, loss 9.13293, acc 0.4375\n",
      "64\n",
      "2017-12-19T22:59:13.479716: step 15, loss 6.95873, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T22:59:14.295739: step 15, loss 1.17746, acc 0.588235\n",
      "\n",
      "64\n",
      "2017-12-19T22:59:15.685983: step 16, loss 8.79932, acc 0.484375\n",
      "56\n",
      "2017-12-19T22:59:16.949871: step 17, loss 6.46712, acc 0.482143\n",
      "64\n",
      "2017-12-19T22:59:18.690686: step 18, loss 7.74444, acc 0.484375\n",
      "64\n",
      "2017-12-19T22:59:20.124330: step 19, loss 6.0421, acc 0.53125\n",
      "64\n",
      "2017-12-19T22:59:21.670726: step 20, loss 4.92055, acc 0.59375\n",
      "64\n",
      "2017-12-19T22:59:23.264479: step 21, loss 7.79738, acc 0.484375\n",
      "64\n",
      "2017-12-19T22:59:24.669529: step 22, loss 5.4972, acc 0.5625\n",
      "64\n",
      "2017-12-19T22:59:26.107494: step 23, loss 8.29389, acc 0.46875\n",
      "64\n",
      "2017-12-19T22:59:27.537875: step 24, loss 8.35011, acc 0.421875\n",
      "64\n",
      "2017-12-19T22:59:28.932881: step 25, loss 6.2464, acc 0.59375\n",
      "64\n",
      "2017-12-19T22:59:30.330636: step 26, loss 6.56811, acc 0.53125\n",
      "64\n",
      "2017-12-19T22:59:31.772643: step 27, loss 6.83138, acc 0.5\n",
      "64\n",
      "2017-12-19T22:59:33.182851: step 28, loss 5.96518, acc 0.59375\n",
      "64\n",
      "2017-12-19T22:59:34.585586: step 29, loss 4.40878, acc 0.5625\n",
      "64\n",
      "2017-12-19T22:59:36.299696: step 30, loss 9.07588, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T22:59:37.167039: step 30, loss 1.3083, acc 0.571429\n",
      "\n",
      "64\n",
      "2017-12-19T22:59:38.627750: step 31, loss 8.53282, acc 0.484375\n",
      "64\n",
      "2017-12-19T22:59:40.466210: step 32, loss 6.6997, acc 0.421875\n",
      "64\n",
      "2017-12-19T22:59:41.860598: step 33, loss 7.24247, acc 0.421875\n",
      "56\n",
      "2017-12-19T22:59:43.166373: step 34, loss 7.55745, acc 0.428571\n",
      "64\n",
      "2017-12-19T22:59:44.559680: step 35, loss 5.3733, acc 0.59375\n",
      "64\n",
      "2017-12-19T22:59:45.954781: step 36, loss 4.10993, acc 0.65625\n",
      "64\n",
      "2017-12-19T22:59:47.363538: step 37, loss 6.64885, acc 0.453125\n",
      "64\n",
      "2017-12-19T22:59:48.751361: step 38, loss 6.66097, acc 0.515625\n",
      "64\n",
      "2017-12-19T22:59:50.348932: step 39, loss 4.38334, acc 0.625\n",
      "64\n",
      "2017-12-19T22:59:51.764764: step 40, loss 5.6583, acc 0.578125\n",
      "64\n",
      "2017-12-19T22:59:53.161436: step 41, loss 4.84277, acc 0.53125\n",
      "64\n",
      "2017-12-19T22:59:54.931660: step 42, loss 3.61349, acc 0.578125\n",
      "64\n",
      "2017-12-19T22:59:57.157090: step 43, loss 2.70099, acc 0.6875\n",
      "64\n",
      "2017-12-19T22:59:58.742899: step 44, loss 6.50534, acc 0.53125\n",
      "64\n",
      "2017-12-19T23:00:00.312125: step 45, loss 4.84435, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:00:01.261781: step 45, loss 4.42618, acc 0.478992\n",
      "\n",
      "64\n",
      "2017-12-19T23:00:02.713980: step 46, loss 3.81987, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:00:04.136468: step 47, loss 4.83943, acc 0.625\n",
      "64\n",
      "2017-12-19T23:00:05.532814: step 48, loss 8.00853, acc 0.46875\n",
      "64\n",
      "2017-12-19T23:00:06.945124: step 49, loss 5.79898, acc 0.53125\n",
      "64\n",
      "2017-12-19T23:00:08.373886: step 50, loss 3.89337, acc 0.640625\n",
      "56\n",
      "2017-12-19T23:00:09.860624: step 51, loss 3.57546, acc 0.607143\n",
      "64\n",
      "2017-12-19T23:00:12.021058: step 52, loss 5.47046, acc 0.515625\n",
      "64\n",
      "2017-12-19T23:00:13.449614: step 53, loss 5.11526, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:14.885237: step 54, loss 5.79532, acc 0.46875\n",
      "64\n",
      "2017-12-19T23:00:16.334293: step 55, loss 5.73672, acc 0.484375\n",
      "64\n",
      "2017-12-19T23:00:17.751890: step 56, loss 5.0893, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:19.171712: step 57, loss 6.24827, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:20.600466: step 58, loss 6.11648, acc 0.546875\n",
      "64\n",
      "2017-12-19T23:00:22.027926: step 59, loss 4.8319, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:00:23.441838: step 60, loss 5.14341, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:00:24.234337: step 60, loss 3.71113, acc 0.487395\n",
      "\n",
      "64\n",
      "2017-12-19T23:00:25.645056: step 61, loss 4.17547, acc 0.625\n",
      "64\n",
      "2017-12-19T23:00:27.065279: step 62, loss 5.70517, acc 0.53125\n",
      "64\n",
      "2017-12-19T23:00:28.491563: step 63, loss 5.37587, acc 0.53125\n",
      "64\n",
      "2017-12-19T23:00:29.904391: step 64, loss 6.41978, acc 0.546875\n",
      "64\n",
      "2017-12-19T23:00:31.368689: step 65, loss 5.11843, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:00:32.800878: step 66, loss 3.31715, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:00:34.236340: step 67, loss 4.07188, acc 0.625\n",
      "56\n",
      "2017-12-19T23:00:35.478898: step 68, loss 6.0474, acc 0.428571\n",
      "64\n",
      "2017-12-19T23:00:37.915460: step 69, loss 3.69675, acc 0.625\n",
      "64\n",
      "2017-12-19T23:00:39.758067: step 70, loss 4.49074, acc 0.625\n",
      "64\n",
      "2017-12-19T23:00:41.648421: step 71, loss 4.32553, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:00:43.469756: step 72, loss 4.13217, acc 0.625\n",
      "64\n",
      "2017-12-19T23:00:44.928396: step 73, loss 3.41296, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:46.359761: step 74, loss 4.11775, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:48.664915: step 75, loss 4.38763, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:00:49.949624: step 75, loss 1.5834, acc 0.554622\n",
      "\n",
      "64\n",
      "2017-12-19T23:00:51.403608: step 76, loss 3.85748, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:00:52.918167: step 77, loss 5.87911, acc 0.546875\n",
      "64\n",
      "2017-12-19T23:00:54.356043: step 78, loss 4.99417, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:55.808338: step 79, loss 4.53868, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:00:57.228898: step 80, loss 5.38359, acc 0.484375\n",
      "64\n",
      "2017-12-19T23:00:58.781913: step 81, loss 4.29285, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:01:00.315682: step 82, loss 3.18182, acc 0.75\n",
      "64\n",
      "2017-12-19T23:01:01.747342: step 83, loss 4.43152, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:01:03.193731: step 84, loss 3.62351, acc 0.59375\n",
      "56\n",
      "2017-12-19T23:01:04.462286: step 85, loss 4.64069, acc 0.535714\n",
      "64\n",
      "2017-12-19T23:01:05.903224: step 86, loss 4.05053, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:01:07.329597: step 87, loss 3.02153, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:01:08.738458: step 88, loss 3.64738, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:01:10.498785: step 89, loss 4.71626, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:01:11.955120: step 90, loss 4.759, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:01:12.748390: step 90, loss 1.01789, acc 0.613445\n",
      "\n",
      "64\n",
      "2017-12-19T23:01:14.166043: step 91, loss 3.41188, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:01:15.587024: step 92, loss 2.91807, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:01:16.995779: step 93, loss 3.99413, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:01:18.421157: step 94, loss 2.50238, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:01:19.846438: step 95, loss 4.16952, acc 0.5\n",
      "64\n",
      "2017-12-19T23:01:21.253429: step 96, loss 5.79349, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:01:22.675064: step 97, loss 4.31728, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:01:24.096642: step 98, loss 4.09576, acc 0.625\n",
      "64\n",
      "2017-12-19T23:01:25.507095: step 99, loss 2.84267, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:01:26.922048: step 100, loss 3.98303, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:01:28.338982: step 101, loss 4.25114, acc 0.59375\n",
      "56\n",
      "2017-12-19T23:01:29.585670: step 102, loss 3.2285, acc 0.553571\n",
      "64\n",
      "2017-12-19T23:01:31.449163: step 103, loss 2.56702, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:01:33.201980: step 104, loss 2.04199, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:01:34.949060: step 105, loss 3.7111, acc 0.59375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-19T23:01:35.867375: step 105, loss 1.33784, acc 0.613445\n",
      "\n",
      "64\n",
      "2017-12-19T23:01:37.276698: step 106, loss 3.19506, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:01:38.869206: step 107, loss 4.21748, acc 0.5625\n",
      "64\n",
      "2017-12-19T23:01:40.279010: step 108, loss 3.38686, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:01:41.709891: step 109, loss 2.66365, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:01:43.124098: step 110, loss 2.3393, acc 0.75\n",
      "64\n",
      "2017-12-19T23:01:44.525042: step 111, loss 3.17306, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:01:45.929296: step 112, loss 1.85566, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:01:47.339785: step 113, loss 3.49839, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:01:48.797390: step 114, loss 2.89247, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:01:50.215235: step 115, loss 2.71748, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:01:51.626302: step 116, loss 2.1478, acc 0.75\n",
      "64\n",
      "2017-12-19T23:01:53.027740: step 117, loss 4.13979, acc 0.5\n",
      "64\n",
      "2017-12-19T23:01:54.492530: step 118, loss 3.28543, acc 0.59375\n",
      "56\n",
      "2017-12-19T23:01:56.110799: step 119, loss 3.37642, acc 0.517857\n",
      "64\n",
      "2017-12-19T23:01:57.816196: step 120, loss 2.67422, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:01:58.614099: step 120, loss 0.908229, acc 0.605042\n",
      "\n",
      "64\n",
      "2017-12-19T23:02:00.121870: step 121, loss 2.85236, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:02:01.772861: step 122, loss 2.24869, acc 0.75\n",
      "64\n",
      "2017-12-19T23:02:03.243087: step 123, loss 3.51124, acc 0.546875\n",
      "64\n",
      "2017-12-19T23:02:04.698244: step 124, loss 2.63391, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:02:06.261609: step 125, loss 1.98921, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:02:08.058960: step 126, loss 2.83869, acc 0.625\n",
      "64\n",
      "2017-12-19T23:02:09.605642: step 127, loss 3.32554, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:02:11.177496: step 128, loss 2.36432, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:02:12.699059: step 129, loss 3.52426, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:02:14.328032: step 130, loss 2.18796, acc 0.625\n",
      "64\n",
      "2017-12-19T23:02:15.752163: step 131, loss 2.54254, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:02:17.158421: step 132, loss 3.00624, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:02:18.550528: step 133, loss 2.80308, acc 0.625\n",
      "64\n",
      "2017-12-19T23:02:20.415407: step 134, loss 2.34161, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:02:22.048738: step 135, loss 2.52488, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:02:22.823934: step 135, loss 0.947905, acc 0.655462\n",
      "\n",
      "56\n",
      "2017-12-19T23:02:24.130585: step 136, loss 2.97438, acc 0.714286\n",
      "64\n",
      "2017-12-19T23:02:25.572040: step 137, loss 3.52971, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:02:26.985580: step 138, loss 1.41645, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:02:28.410290: step 139, loss 2.81285, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:02:29.815095: step 140, loss 2.14546, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:02:31.245855: step 141, loss 2.59619, acc 0.625\n",
      "64\n",
      "2017-12-19T23:02:32.666771: step 142, loss 2.48845, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:02:34.085152: step 143, loss 2.83928, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:02:35.493415: step 144, loss 2.74033, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:02:36.894104: step 145, loss 1.75266, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:02:38.288561: step 146, loss 3.34065, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:02:39.701828: step 147, loss 2.80951, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:02:41.109291: step 148, loss 2.43106, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:02:42.529651: step 149, loss 4.10507, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:02:43.962579: step 150, loss 1.89594, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:02:44.761333: step 150, loss 0.901489, acc 0.638655\n",
      "\n",
      "64\n",
      "2017-12-19T23:02:46.155151: step 151, loss 3.38783, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:02:47.565440: step 152, loss 3.53221, acc 0.578125\n",
      "56\n",
      "2017-12-19T23:02:48.809446: step 153, loss 2.64725, acc 0.589286\n",
      "64\n",
      "2017-12-19T23:02:50.229915: step 154, loss 3.16436, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:02:51.639477: step 155, loss 2.38132, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:02:53.134945: step 156, loss 2.09275, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:02:55.162813: step 157, loss 2.61214, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:02:57.069958: step 158, loss 2.05966, acc 0.75\n",
      "64\n",
      "2017-12-19T23:02:58.487113: step 159, loss 3.39053, acc 0.546875\n",
      "64\n",
      "2017-12-19T23:02:59.885369: step 160, loss 2.79323, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:03:01.315409: step 161, loss 3.89212, acc 0.546875\n",
      "64\n",
      "2017-12-19T23:03:02.722837: step 162, loss 2.24825, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:03:04.135278: step 163, loss 1.53252, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:03:05.550659: step 164, loss 2.18096, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:03:06.963604: step 165, loss 1.5142, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:03:07.747397: step 165, loss 1.17934, acc 0.638655\n",
      "\n",
      "64\n",
      "2017-12-19T23:03:09.164795: step 166, loss 0.783032, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:03:10.577356: step 167, loss 1.86314, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:03:11.998556: step 168, loss 2.00347, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:03:13.419668: step 169, loss 1.90209, acc 0.671875\n",
      "56\n",
      "2017-12-19T23:03:14.660013: step 170, loss 1.56455, acc 0.75\n",
      "64\n",
      "2017-12-19T23:03:16.062980: step 171, loss 1.59983, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:03:17.460961: step 172, loss 1.87335, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:03:18.880568: step 173, loss 1.81216, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:03:20.289977: step 174, loss 1.79689, acc 0.75\n",
      "64\n",
      "2017-12-19T23:03:21.689244: step 175, loss 2.95712, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:03:23.125906: step 176, loss 2.05532, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:03:24.538416: step 177, loss 2.09362, acc 0.75\n",
      "64\n",
      "2017-12-19T23:03:25.950200: step 178, loss 1.7177, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:03:27.377004: step 179, loss 2.34526, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:03:28.794048: step 180, loss 1.85971, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:03:29.573086: step 180, loss 1.03874, acc 0.638655\n",
      "\n",
      "64\n",
      "2017-12-19T23:03:31.004871: step 181, loss 2.05896, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:03:32.424807: step 182, loss 1.59916, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:03:33.830712: step 183, loss 2.32118, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:03:35.251168: step 184, loss 1.40709, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:03:36.670331: step 185, loss 2.37277, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:03:38.078566: step 186, loss 1.68611, acc 0.671875\n",
      "56\n",
      "2017-12-19T23:03:39.328704: step 187, loss 1.55095, acc 0.660714\n",
      "64\n",
      "2017-12-19T23:03:40.741565: step 188, loss 1.90092, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:03:42.161188: step 189, loss 2.13602, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:03:43.587344: step 190, loss 2.22991, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:03:45.006602: step 191, loss 2.78289, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:03:46.418062: step 192, loss 2.25145, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:03:47.841229: step 193, loss 2.04898, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:03:49.255341: step 194, loss 2.18316, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:03:50.677640: step 195, loss 1.77694, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:03:51.469595: step 195, loss 1.57016, acc 0.571429\n",
      "\n",
      "64\n",
      "2017-12-19T23:03:52.887603: step 196, loss 1.04319, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:03:54.300827: step 197, loss 1.15797, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:03:55.770078: step 198, loss 2.2846, acc 0.625\n",
      "64\n",
      "2017-12-19T23:03:57.186119: step 199, loss 1.07913, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:03:58.620660: step 200, loss 2.59048, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:04:00.042137: step 201, loss 2.17725, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:04:01.473260: step 202, loss 1.45986, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:04:02.880670: step 203, loss 2.59559, acc 0.671875\n",
      "56\n",
      "2017-12-19T23:04:04.157198: step 204, loss 1.87815, acc 0.678571\n",
      "64\n",
      "2017-12-19T23:04:05.577935: step 205, loss 2.68879, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:04:06.992860: step 206, loss 1.8839, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:04:08.397073: step 207, loss 1.7601, acc 0.625\n",
      "64\n",
      "2017-12-19T23:04:09.817078: step 208, loss 1.6704, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:04:11.228097: step 209, loss 1.22165, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:04:12.641935: step 210, loss 1.97764, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:04:13.442407: step 210, loss 0.961311, acc 0.630252\n",
      "\n",
      "64\n",
      "2017-12-19T23:04:14.867351: step 211, loss 1.11045, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:04:16.281716: step 212, loss 1.40764, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:04:17.700155: step 213, loss 1.10654, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:04:19.116556: step 214, loss 2.3044, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:04:20.533014: step 215, loss 1.05793, acc 0.75\n",
      "64\n",
      "2017-12-19T23:04:21.945506: step 216, loss 1.52105, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:04:23.366175: step 217, loss 1.56125, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:04:24.795616: step 218, loss 1.02908, acc 0.78125\n",
      "64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-19T23:04:26.214656: step 219, loss 2.02729, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:04:27.622560: step 220, loss 1.52452, acc 0.71875\n",
      "56\n",
      "2017-12-19T23:04:28.854492: step 221, loss 1.73108, acc 0.660714\n",
      "64\n",
      "2017-12-19T23:04:30.314882: step 222, loss 1.57131, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:04:32.324106: step 223, loss 2.48689, acc 0.59375\n",
      "64\n",
      "2017-12-19T23:04:34.258980: step 224, loss 1.77456, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:04:36.294096: step 225, loss 1.59048, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:04:37.523567: step 225, loss 0.926494, acc 0.630252\n",
      "\n",
      "64\n",
      "2017-12-19T23:04:39.173638: step 226, loss 2.24946, acc 0.578125\n",
      "64\n",
      "2017-12-19T23:04:41.121698: step 227, loss 1.51531, acc 0.8125\n",
      "64\n",
      "2017-12-19T23:04:42.564916: step 228, loss 1.76839, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:04:43.993390: step 229, loss 1.25184, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:04:45.421056: step 230, loss 0.802777, acc 0.8125\n",
      "64\n",
      "2017-12-19T23:04:46.843680: step 231, loss 1.70929, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:04:48.254812: step 232, loss 1.50413, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:04:49.664374: step 233, loss 1.17822, acc 0.84375\n",
      "64\n",
      "2017-12-19T23:04:51.083044: step 234, loss 2.39369, acc 0.609375\n",
      "64\n",
      "2017-12-19T23:04:52.579627: step 235, loss 1.4789, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:04:53.973860: step 236, loss 1.15682, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:04:55.437659: step 237, loss 1.52558, acc 0.734375\n",
      "56\n",
      "2017-12-19T23:04:56.683424: step 238, loss 0.910859, acc 0.785714\n",
      "64\n",
      "2017-12-19T23:04:58.088542: step 239, loss 1.39191, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:04:59.507460: step 240, loss 1.03111, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:05:00.301833: step 240, loss 0.812038, acc 0.613445\n",
      "\n",
      "64\n",
      "2017-12-19T23:05:01.727643: step 241, loss 1.79855, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:05:03.161385: step 242, loss 2.0181, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:05:04.596098: step 243, loss 1.58538, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:05:06.003655: step 244, loss 1.04812, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:05:07.419000: step 245, loss 0.972856, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:05:08.830509: step 246, loss 1.48943, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:05:10.238111: step 247, loss 1.55999, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:05:11.652852: step 248, loss 1.61521, acc 0.75\n",
      "64\n",
      "2017-12-19T23:05:13.044900: step 249, loss 0.757405, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:05:14.457915: step 250, loss 1.63574, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:05:15.896716: step 251, loss 1.19496, acc 0.75\n",
      "64\n",
      "2017-12-19T23:05:17.798858: step 252, loss 1.51473, acc 0.75\n",
      "64\n",
      "2017-12-19T23:05:19.695838: step 253, loss 1.59869, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:05:21.520637: step 254, loss 1.4337, acc 0.6875\n",
      "56\n",
      "2017-12-19T23:05:23.113073: step 255, loss 1.39645, acc 0.642857\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:05:23.907975: step 255, loss 0.822143, acc 0.579832\n",
      "\n",
      "64\n",
      "2017-12-19T23:05:25.320438: step 256, loss 1.31984, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:05:26.797549: step 257, loss 1.44362, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:05:28.236888: step 258, loss 1.21368, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:05:29.653866: step 259, loss 1.75365, acc 0.671875\n",
      "64\n",
      "2017-12-19T23:05:31.096685: step 260, loss 1.35204, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:05:32.526000: step 261, loss 2.25017, acc 0.6875\n",
      "64\n",
      "2017-12-19T23:05:33.949941: step 262, loss 0.869837, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:05:35.362836: step 263, loss 1.09178, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:05:36.778389: step 264, loss 1.63014, acc 0.640625\n",
      "64\n",
      "2017-12-19T23:05:38.192591: step 265, loss 1.40256, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:05:39.608771: step 266, loss 1.06081, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:05:41.015877: step 267, loss 0.992511, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:05:42.429141: step 268, loss 1.26213, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:05:43.842449: step 269, loss 0.996502, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:05:45.255475: step 270, loss 1.18168, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:05:46.037774: step 270, loss 0.81857, acc 0.613445\n",
      "\n",
      "64\n",
      "2017-12-19T23:05:47.451900: step 271, loss 1.31121, acc 0.671875\n",
      "56\n",
      "2017-12-19T23:05:48.715196: step 272, loss 1.18276, acc 0.732143\n",
      "64\n",
      "2017-12-19T23:05:50.143259: step 273, loss 1.21573, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:05:51.579411: step 274, loss 1.17161, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:05:53.019669: step 275, loss 0.546378, acc 0.8125\n",
      "64\n",
      "2017-12-19T23:05:54.451823: step 276, loss 0.825089, acc 0.828125\n",
      "64\n",
      "2017-12-19T23:05:55.885950: step 277, loss 1.25302, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:05:57.326158: step 278, loss 0.700458, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:05:58.764199: step 279, loss 0.760999, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:06:00.213255: step 280, loss 1.03912, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:06:01.727162: step 281, loss 1.20267, acc 0.75\n",
      "64\n",
      "2017-12-19T23:06:03.183449: step 282, loss 1.51472, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:06:04.630222: step 283, loss 1.07931, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:06:06.075030: step 284, loss 0.907396, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:06:08.486700: step 285, loss 1.3532, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:06:09.579388: step 285, loss 0.825398, acc 0.605042\n",
      "\n",
      "64\n",
      "2017-12-19T23:06:11.359736: step 286, loss 0.997587, acc 0.75\n",
      "64\n",
      "2017-12-19T23:06:12.836320: step 287, loss 1.05572, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:06:14.255271: step 288, loss 0.844169, acc 0.703125\n",
      "56\n",
      "2017-12-19T23:06:15.528821: step 289, loss 1.14094, acc 0.714286\n",
      "64\n",
      "2017-12-19T23:06:16.973862: step 290, loss 0.803273, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:06:18.399050: step 291, loss 0.779298, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:06:19.844671: step 292, loss 1.06438, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:06:21.277709: step 293, loss 1.00328, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:06:22.712162: step 294, loss 1.08142, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:06:24.147092: step 295, loss 0.981748, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:06:25.573687: step 296, loss 0.993698, acc 0.78125\n",
      "64\n",
      "2017-12-19T23:06:27.015608: step 297, loss 0.901782, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:06:28.461266: step 298, loss 1.09334, acc 0.75\n",
      "64\n",
      "2017-12-19T23:06:29.879586: step 299, loss 1.02311, acc 0.71875\n",
      "64\n",
      "2017-12-19T23:06:31.316776: step 300, loss 1.85045, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T23:06:32.104364: step 300, loss 0.816531, acc 0.588235\n",
      "\n",
      "64\n",
      "2017-12-19T23:06:33.529495: step 301, loss 0.902876, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:06:34.977839: step 302, loss 1.19589, acc 0.65625\n",
      "64\n",
      "2017-12-19T23:06:36.416450: step 303, loss 0.582079, acc 0.875\n",
      "64\n",
      "2017-12-19T23:06:37.835643: step 304, loss 1.0102, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:06:39.267069: step 305, loss 0.96972, acc 0.796875\n",
      "56\n",
      "2017-12-19T23:06:40.516805: step 306, loss 0.629782, acc 0.767857\n",
      "64\n",
      "2017-12-19T23:06:41.925970: step 307, loss 1.10605, acc 0.765625\n",
      "64\n",
      "2017-12-19T23:06:43.356030: step 308, loss 0.659285, acc 0.734375\n",
      "64\n",
      "2017-12-19T23:06:44.764188: step 309, loss 0.487432, acc 0.890625\n",
      "64\n",
      "2017-12-19T23:06:46.172790: step 310, loss 1.11664, acc 0.75\n",
      "64\n",
      "2017-12-19T23:06:47.589820: step 311, loss 1.11376, acc 0.703125\n",
      "64\n",
      "2017-12-19T23:06:49.004192: step 312, loss 1.14759, acc 0.75\n",
      "64\n",
      "2017-12-19T23:06:50.418222: step 313, loss 0.924945, acc 0.796875\n",
      "64\n",
      "2017-12-19T23:06:51.850910: step 314, loss 1.32454, acc 0.703125\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from cnn_model import CNN\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=ALLOW_SOFT_PLACEMENT,\n",
    "      log_device_placement=LOG_DEVICE_PLACEMENT)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            filter_size=FILTER_SIZES,\n",
    "            num_filters=NUM_FILTERS,\n",
    "            l2_reg_lambda=L2_REG_LAMBDA)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), BATCH_SIZE, NUM_EPOCHS)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            print(len(x_batch))\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev)\n",
    "                print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
