{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "ALLOW_SOFT_PLACEMENT=True\n",
    "BATCH_SIZE=64\n",
    "DEV_SAMPLE_PERCENTAGE=0.1\n",
    "DROPOUT_KEEP_PROB=0.5\n",
    "EMBEDDING_DIM=64\n",
    "EVALUATE_EVERY=15\n",
    "FILTER_SIZES=[3,4,5]\n",
    "L2_REG_LAMBDA=0.001\n",
    "LOG_DEVICE_PLACEMENT=False\n",
    "NUM_EPOCHS=20\n",
    "NUM_FILTERS=128\n",
    "fname = \"w2vmodel\" #w2vmodel128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def washstr(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    lower_txt = washstr(text)\n",
    "    return lower_txt.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extfeat(X_train):\n",
    "    neg = pd.read_csv('negative-words.txt')['neg'].values\n",
    "    pos = pd.read_csv('positive-words.txt')['pos'].values\n",
    "    neg = set(neg)\n",
    "    pos = set(pos)\n",
    "    \n",
    "    ext = []\n",
    "    for i in range(len(X_train)):\n",
    "        p = 0.0\n",
    "        n = 0.0\n",
    "        total = 0.0\n",
    "        for j in X_train[i]:\n",
    "            if j != \" \":\n",
    "                total += 1\n",
    "                if j in pos:\n",
    "                    p += 1\n",
    "                if j in neg:\n",
    "                    n += 1\n",
    "        if n == 0:\n",
    "            ratio = p\n",
    "        else:\n",
    "            ratio = p/n\n",
    "        ext.append([p,n,p/total,n/total,ratio])\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def word2vec(X_train):\n",
    "    X = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X.append(tokenizer(X_train[i]))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        while len(X[i]) < max_len:\n",
    "            X[i].append(\" \")\n",
    "        \n",
    "    model = gensim.models.Word2Vec(X, min_count=1,size=EMBEDDING_DIM)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwvec(X_train,model):\n",
    "    x = []\n",
    "    for i in range(len(X_train)):\n",
    "        x.append(model.wv[X_train[i]])\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    print(data_size)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: DROPOUT_KEEP_PROB\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "def dev_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('labeledmusic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lytrain = train_set['text'].values \n",
    "\n",
    "y_train = train_set['mood'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,) (1199,)\n"
     ]
    }
   ],
   "source": [
    "print(lytrain.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for i in range(lytrain.shape[0]):\n",
    "    X_train.append(tokenizer(lytrain[i]))\n",
    "    max_len = max(max_len, len(X_train[i]))\n",
    "\n",
    "    if y_train[i] == \"sad\":\n",
    "        Y.append([0,1])\n",
    "    else:\n",
    "        Y.append([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199 1266\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill each lyric, to make it of 1087 tokens\n",
    "for i in range(len(X_train)):\n",
    "    while len(X_train[i]) < max_len:\n",
    "        X_train[i].append(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if len(X_train[i]) != max_len:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,)\n"
     ]
    }
   ],
   "source": [
    "print(lytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since the dataset is too small, word vector may not be good, so use a greater lyric set to generate w2v model\n",
    "# unlabeled = pd.read_csv('songdata.csv')\n",
    "# w2vtran = unlabeled['text'].values\n",
    "# w2vtran = np.row_stack((w2vtran.reshape(-1,1),lytrain.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = word2vec(w2vtran[:,0])\n",
    "# model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2vtran.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1266, 64)\n"
     ]
    }
   ],
   "source": [
    "x_ = getwvec(X_train,model)\n",
    "print(x_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra = extfeat(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 5)\n"
     ]
    }
   ],
   "source": [
    "# extra = np.array(extra)\n",
    "# print(extra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ = x_.reshape(-1,x_.shape[1]*x_.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.column_stack((x_,extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA \n",
    "# pca=PCA(n_components=0.998)\n",
    "# x = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1266, 64) (1199, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yTrain/Dev split: 1080/119\n",
      "xTrain/Dev split: 1080/119\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(DEV_SAMPLE_PERCENTAGE * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"yTrain/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "print(\"xTrain/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "1080\n",
      "64\n",
      "2017-12-19T19:06:13.543205: step 1, loss 8.52014, acc 0.5\n",
      "64\n",
      "2017-12-19T19:06:15.606383: step 2, loss 7.69951, acc 0.5\n",
      "64\n",
      "2017-12-19T19:06:17.877989: step 3, loss 11.933, acc 0.3125\n",
      "64\n",
      "2017-12-19T19:06:20.820312: step 4, loss 7.24847, acc 0.5\n",
      "64\n",
      "2017-12-19T19:06:23.747184: step 5, loss 7.01812, acc 0.453125\n",
      "64\n",
      "2017-12-19T19:06:26.191727: step 6, loss 7.16149, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:06:29.024094: step 7, loss 5.8822, acc 0.484375\n",
      "64\n",
      "2017-12-19T19:06:31.739036: step 8, loss 7.19169, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:06:34.826998: step 9, loss 7.4296, acc 0.484375\n",
      "64\n",
      "2017-12-19T19:06:37.655452: step 10, loss 7.14848, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:06:40.146333: step 11, loss 10.2596, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:06:42.240236: step 12, loss 7.73342, acc 0.5\n",
      "64\n",
      "2017-12-19T19:06:44.373337: step 13, loss 6.62298, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:06:46.896291: step 14, loss 10.4869, acc 0.5\n",
      "64\n",
      "2017-12-19T19:06:49.748203: step 15, loss 8.65076, acc 0.46875\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:06:51.278155: step 15, loss 1.18519, acc 0.546219\n",
      "\n",
      "64\n",
      "2017-12-19T19:06:53.689222: step 16, loss 9.23247, acc 0.4375\n",
      "56\n",
      "2017-12-19T19:06:55.866292: step 17, loss 7.98612, acc 0.517857\n",
      "64\n",
      "2017-12-19T19:06:58.524492: step 18, loss 6.0558, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:07:00.958727: step 19, loss 10.3695, acc 0.375\n",
      "64\n",
      "2017-12-19T19:07:03.401578: step 20, loss 8.58891, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:07:06.126525: step 21, loss 8.44795, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:07:08.727101: step 22, loss 7.56003, acc 0.34375\n",
      "64\n",
      "2017-12-19T19:07:11.406432: step 23, loss 5.45715, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:07:14.503649: step 24, loss 5.00333, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:07:17.513943: step 25, loss 7.30835, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:07:20.081417: step 26, loss 6.36456, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:07:22.571155: step 27, loss 4.67377, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:07:24.891549: step 28, loss 6.06283, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:07:27.101013: step 29, loss 4.34878, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:07:29.963605: step 30, loss 5.36341, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:07:31.457287: step 30, loss 1.8826, acc 0.495798\n",
      "\n",
      "64\n",
      "2017-12-19T19:07:33.645693: step 31, loss 6.72204, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:07:35.881987: step 32, loss 8.4094, acc 0.421875\n",
      "64\n",
      "2017-12-19T19:07:38.199066: step 33, loss 7.32584, acc 0.5\n",
      "56\n",
      "2017-12-19T19:07:40.148776: step 34, loss 3.5349, acc 0.660714\n",
      "64\n",
      "2017-12-19T19:07:42.278886: step 35, loss 6.39946, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:07:44.479523: step 36, loss 4.66075, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:07:46.603437: step 37, loss 8.3954, acc 0.34375\n",
      "64\n",
      "2017-12-19T19:07:49.090294: step 38, loss 4.25226, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:07:51.219020: step 39, loss 4.10946, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:07:53.366127: step 40, loss 5.3031, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:07:55.489274: step 41, loss 6.34589, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:07:57.584019: step 42, loss 3.28659, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:07:59.911139: step 43, loss 5.63223, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:08:02.012396: step 44, loss 6.01101, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:08:04.116288: step 45, loss 6.677, acc 0.453125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:08:05.279930: step 45, loss 3.78816, acc 0.487395\n",
      "\n",
      "64\n",
      "2017-12-19T19:08:07.365786: step 46, loss 3.75737, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:08:09.685799: step 47, loss 7.18863, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:08:11.806387: step 48, loss 5.73735, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:08:13.906176: step 49, loss 4.40721, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:08:16.004208: step 50, loss 3.97764, acc 0.609375\n",
      "56\n",
      "2017-12-19T19:08:17.925240: step 51, loss 4.94015, acc 0.589286\n",
      "64\n",
      "2017-12-19T19:08:20.226371: step 52, loss 5.97446, acc 0.5\n",
      "64\n",
      "2017-12-19T19:08:22.333220: step 53, loss 5.50037, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:08:24.431335: step 54, loss 5.00588, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:08:26.527547: step 55, loss 5.5625, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:08:28.887428: step 56, loss 5.59567, acc 0.484375\n",
      "64\n",
      "2017-12-19T19:08:31.015750: step 57, loss 3.39552, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:08:33.131866: step 58, loss 5.86923, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:08:35.212269: step 59, loss 5.39911, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:08:37.323095: step 60, loss 3.11082, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:08:38.737147: step 60, loss 1.16933, acc 0.554622\n",
      "\n",
      "64\n",
      "2017-12-19T19:08:41.188970: step 61, loss 4.34235, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:08:44.479764: step 62, loss 3.64248, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:08:47.900728: step 63, loss 4.33966, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:08:50.600113: step 64, loss 4.1784, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:08:52.748811: step 65, loss 5.24355, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:08:54.898955: step 66, loss 5.54893, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:08:56.996406: step 67, loss 6.25393, acc 0.546875\n",
      "56\n",
      "2017-12-19T19:08:59.047119: step 68, loss 3.8463, acc 0.660714\n",
      "64\n",
      "2017-12-19T19:09:01.164723: step 69, loss 3.59301, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:09:03.249710: step 70, loss 4.92085, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:09:05.461087: step 71, loss 2.93839, acc 0.671875\n",
      "64\n",
      "2017-12-19T19:09:07.983215: step 72, loss 5.34715, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:09:10.813111: step 73, loss 3.39586, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:09:12.959521: step 74, loss 3.96388, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:09:15.140865: step 75, loss 4.10984, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:09:16.345460: step 75, loss 1.14051, acc 0.630252\n",
      "\n",
      "64\n",
      "2017-12-19T19:09:18.756132: step 76, loss 5.70873, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:09:21.185344: step 77, loss 4.50233, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:09:23.318734: step 78, loss 4.93561, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:09:25.453073: step 79, loss 4.97261, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:09:27.586412: step 80, loss 2.89152, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:09:29.906688: step 81, loss 3.94896, acc 0.5625\n",
      "64\n",
      "2017-12-19T19:09:32.019771: step 82, loss 5.52421, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:09:34.349701: step 83, loss 5.77657, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:09:36.469548: step 84, loss 5.56323, acc 0.5625\n",
      "56\n",
      "2017-12-19T19:09:39.240998: step 85, loss 3.37938, acc 0.660714\n",
      "64\n",
      "2017-12-19T19:09:42.742119: step 86, loss 4.9051, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:09:46.141738: step 87, loss 3.20912, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:09:50.657315: step 88, loss 4.68641, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:09:53.997673: step 89, loss 3.31578, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:09:56.581535: step 90, loss 2.98339, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:09:57.831312: step 90, loss 1.01645, acc 0.605042\n",
      "\n",
      "64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-19T19:10:01.005763: step 91, loss 3.78621, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:10:03.719304: step 92, loss 2.81587, acc 0.625\n",
      "64\n",
      "2017-12-19T19:10:06.795294: step 93, loss 3.32598, acc 0.625\n",
      "64\n",
      "2017-12-19T19:10:10.363238: step 94, loss 4.12385, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:10:12.541305: step 95, loss 5.51853, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:10:15.190520: step 96, loss 3.93596, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:10:17.536222: step 97, loss 3.97499, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:10:21.010494: step 98, loss 4.30489, acc 0.515625\n",
      "64\n",
      "2017-12-19T19:10:23.490726: step 99, loss 4.02286, acc 0.625\n",
      "64\n",
      "2017-12-19T19:10:26.586434: step 100, loss 3.13327, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:10:29.250946: step 101, loss 2.88886, acc 0.609375\n",
      "56\n",
      "2017-12-19T19:10:31.426427: step 102, loss 4.00144, acc 0.571429\n",
      "64\n",
      "2017-12-19T19:10:34.014450: step 103, loss 3.96465, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:10:36.665297: step 104, loss 1.81149, acc 0.71875\n",
      "64\n",
      "2017-12-19T19:10:39.366430: step 105, loss 2.48965, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:10:40.953844: step 105, loss 2.33652, acc 0.529412\n",
      "\n",
      "64\n",
      "2017-12-19T19:10:43.978948: step 106, loss 3.1652, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:10:47.303889: step 107, loss 4.07294, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:10:49.874791: step 108, loss 4.17931, acc 0.46875\n",
      "64\n",
      "2017-12-19T19:10:52.420114: step 109, loss 3.94587, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:10:55.810867: step 110, loss 2.5153, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:10:58.782024: step 111, loss 2.33602, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:11:01.630922: step 112, loss 3.03994, acc 0.625\n",
      "64\n",
      "2017-12-19T19:11:05.143773: step 113, loss 2.45852, acc 0.6875\n",
      "64\n",
      "2017-12-19T19:11:07.989145: step 114, loss 2.35673, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:11:11.433514: step 115, loss 3.54645, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:11:13.804039: step 116, loss 3.29182, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:11:15.951312: step 117, loss 3.81061, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:11:18.173795: step 118, loss 2.93679, acc 0.515625\n",
      "56\n",
      "2017-12-19T19:11:20.132022: step 119, loss 3.48626, acc 0.625\n",
      "64\n",
      "2017-12-19T19:11:22.234491: step 120, loss 2.7396, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:11:23.369458: step 120, loss 1.80568, acc 0.546219\n",
      "\n",
      "64\n",
      "2017-12-19T19:11:25.440686: step 121, loss 3.17432, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:11:27.538837: step 122, loss 3.10211, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:11:29.847425: step 123, loss 2.72064, acc 0.734375\n",
      "64\n",
      "2017-12-19T19:11:31.950124: step 124, loss 3.27222, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:11:34.034210: step 125, loss 2.1283, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:11:36.112318: step 126, loss 2.75165, acc 0.65625\n",
      "64\n",
      "2017-12-19T19:11:38.348933: step 127, loss 3.26386, acc 0.53125\n",
      "64\n",
      "2017-12-19T19:11:40.524282: step 128, loss 2.89772, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:11:42.608514: step 129, loss 2.82767, acc 0.65625\n",
      "64\n",
      "2017-12-19T19:11:44.700768: step 130, loss 3.15067, acc 0.625\n",
      "64\n",
      "2017-12-19T19:11:46.791860: step 131, loss 2.43631, acc 0.65625\n",
      "64\n",
      "2017-12-19T19:11:49.126618: step 132, loss 2.4125, acc 0.609375\n",
      "64\n",
      "2017-12-19T19:11:51.214408: step 133, loss 2.61218, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:11:53.478296: step 134, loss 2.54283, acc 0.6875\n",
      "64\n",
      "2017-12-19T19:11:55.680666: step 135, loss 2.14781, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:11:56.878140: step 135, loss 3.41457, acc 0.512605\n",
      "\n",
      "56\n",
      "2017-12-19T19:11:58.978245: step 136, loss 3.0531, acc 0.625\n",
      "64\n",
      "2017-12-19T19:12:01.342679: step 137, loss 2.85965, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:12:03.686277: step 138, loss 2.06582, acc 0.78125\n",
      "64\n",
      "2017-12-19T19:12:06.097130: step 139, loss 3.36621, acc 0.625\n",
      "64\n",
      "2017-12-19T19:12:10.091764: step 140, loss 2.27798, acc 0.671875\n",
      "64\n",
      "2017-12-19T19:12:12.489699: step 141, loss 2.17068, acc 0.671875\n",
      "64\n",
      "2017-12-19T19:12:14.679390: step 142, loss 2.16409, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:12:16.919670: step 143, loss 2.04705, acc 0.6875\n",
      "64\n",
      "2017-12-19T19:12:19.902984: step 144, loss 2.54029, acc 0.65625\n",
      "64\n",
      "2017-12-19T19:12:22.332406: step 145, loss 1.52508, acc 0.75\n",
      "64\n",
      "2017-12-19T19:12:24.561841: step 146, loss 1.74607, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:12:26.735678: step 147, loss 3.4304, acc 0.578125\n",
      "64\n",
      "2017-12-19T19:12:29.813032: step 148, loss 1.64555, acc 0.59375\n",
      "64\n",
      "2017-12-19T19:12:32.111603: step 149, loss 1.90449, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:12:34.890307: step 150, loss 2.47423, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-12-19T19:12:36.291505: step 150, loss 1.17598, acc 0.546219\n",
      "\n",
      "64\n",
      "2017-12-19T19:12:39.148139: step 151, loss 2.31085, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:12:41.318036: step 152, loss 3.47178, acc 0.5\n",
      "56\n",
      "2017-12-19T19:12:43.288365: step 153, loss 2.22014, acc 0.660714\n",
      "64\n",
      "2017-12-19T19:12:45.462232: step 154, loss 3.94478, acc 0.546875\n",
      "64\n",
      "2017-12-19T19:12:47.875385: step 155, loss 2.62852, acc 0.625\n",
      "64\n",
      "2017-12-19T19:12:50.662979: step 156, loss 2.08509, acc 0.703125\n",
      "64\n",
      "2017-12-19T19:12:53.078436: step 157, loss 3.20281, acc 0.65625\n",
      "64\n",
      "2017-12-19T19:12:56.243529: step 158, loss 2.01193, acc 0.640625\n",
      "64\n",
      "2017-12-19T19:12:59.102594: step 159, loss 2.64625, acc 0.703125\n",
      "64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8d08ac1601ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVALUATE_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4dce8bec920d>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m      7\u001b[0m     _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         feed_dict)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from text_cnn import TextCNN\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=ALLOW_SOFT_PLACEMENT,\n",
    "      log_device_placement=LOG_DEVICE_PLACEMENT)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            filter_sizes=FILTER_SIZES,\n",
    "            num_filters=NUM_FILTERS,\n",
    "            l2_reg_lambda=L2_REG_LAMBDA)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), BATCH_SIZE, NUM_EPOCHS)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            print(len(x_batch))\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev)\n",
    "                print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
